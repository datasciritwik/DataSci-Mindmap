# DataSci-Mindmap

A comprehensive Data Science Mindmap for quick revision.

### ðŸš€ Deployment

[![Streamlit App](https://img.shields.io/badge/Streamlit-Live%20App-brightgreen?logo=streamlit)](https://aimlnotes.streamlit.app/)

ðŸ”— **[Click here to open the AIML Notes App](https://aimlnotes.streamlit.app/)**

## Table of Contents
- [Code Examples](#code-examples)
- [Detailed Discussion](#detailed-discussion)
- [Project Examples](#project-examples)
- [Flow Diagram Mindmaps](#flow-diagram-mindmaps)
- [Contributing](#contributing)
- [License](#license)

## Code Examples
Each topic includes practical code examples in Python, covering:
- Model implementation
- Data preprocessing
- Evaluation metrics
- Visualization

Explore the respective markdown files in each folder for hands-on code snippets.

## Detailed Discussion
In-depth explanations are provided for each algorithm and concept, including:
- Mathematical intuition
- Use cases
- Pros and cons
- Hyperparameters

Refer to the markdown files for detailed notes and references.

## Project Examples
Sample end-to-end project outlines and mini-projects are included to demonstrate real-world applications of:
- Classification
- Regression
- Unsupervised learning
- Loss functions

## Flow Diagram Mindmaps
For each topic, a flow diagram or mindmap is provided to visually summarize:
- Key concepts
- Relationships between algorithms
- Decision trees for model selection

Mindmaps are available as images or ASCII diagrams in the respective markdown files.

## Contributing
Contributions are welcome! To contribute:
- Fork the repository
- Create a new branch
- Add or update markdown files, code, or diagrams
- Submit a pull request

Please see the [CONTRIBUTING.md](CONTRIBUTING.md) for more details (or create one if it does not exist).

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Project Structure

- **LossFunctions/**: Various loss functions used in machine learning, including:
  - Classification Loss Functions
  - Regression Loss Functions
  - Contrastive & Self-Supervised Losses
  - GAN & Generative Model Losses
  - Object Detection Losses
  - Ranking & Learning-to-Rank Losses
  - Reinforcement Learning Losses
  - Segmentation & Pixel-wise Losses

- **ML-Classification-Models/**: Collection of classification models, such as:
  - Decision Trees, Random Forest, Extra Trees
  - Logistic Regression, Ridge Classifier, SGDClassifier
  - Naive Bayes (Bernoulli, Gaussian, Multinomial, Complement)
  - K-Nearest Neighbors, Radius Neighbors
  - SVM, One-Class SVM
  - Ensemble Methods (Bagging, Boosting, Stacking, Voting)
  - Neural Networks (MLP, Deep Neural Networks, TabNet, Transformer-Based)
  - Outlier Detection (Isolation Forest, Local Outlier Factor)
  - Others (Bayesian Network, Rule-Based, etc.)

- **ML-Regression-Models/**: Collection of regression models, including:
  - Linear, Ridge, Lasso, ElasticNet, Polynomial, Quantile, Huber, Theil-Sen, Tweedie
  - Decision Tree, Random Forest, Extra Trees, Gradient Boosting, AdaBoost, Bagging
  - Bayesian Methods (Bayesian Ridge, Bayesian Neural Networks)
  - K-Nearest Neighbors, Isotonic, RANSAC, LARS
  - Support Vector Regression, TabNet, XGBoost, LightGBM, CatBoost
  - Stacking, Voting

- **ML-Unsupervised-Models/**: Unsupervised learning models and algorithms:
  - Clustering (K-Means, K-Medoids, Agglomerative, Divisive, BIRCH, DBSCAN, HDBSCAN, OPTICS, Mini-Batch K-Means)
  - Dimensionality Reduction (PCA, Kernel PCA, Truncated SVD, UMAP, t-SNE, LSA, ICA, NMF, MDS, Isomap, LLE)
  - Association Rule Mining (Apriori, ECLAT, FP-Growth)
  - Gaussian Mixture Models, Bayesian Gaussian Mixture
  - Autoencoders

## Usage

Browse the markdown files in each folder for concise notes and mindmaps on each topic. Ideal for quick revision and interview preparation.

---

*Contributions welcome!*
